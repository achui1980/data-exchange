# Light Data Exchange （轻量数据交换平台）

## 设计思路
- 将数据处理分为两部分，一分半是数据生产者，一部分是数据消费者（生产者消费者模型），这么做的好处可以把数据抓取和数据处理解耦，将原来的单机压力分散到多台。消费者模块部署在云端平台上，可以实现弹性扩展，提高数据处理能力。生成者模块由于只负责数据的解析和送消息队列， 梳理逻辑相对简单，因此可以指定一定数量的服务器进行部署，比如两台服务器。
- 生产者数据解析端可以接入各种类型的数据格式CSV，Excel，RestAPI， XML，Json，爬虫所产生的的数据 等等，数据源的存放可以是SFTP，文件系统等。爬虫系统的接入，可以通过爬虫系统爬取生成数据文件后，放入文件系统,SFTP存储等
- 因为是批数据处理，所以在通过Kafka的时候，也需要保证消息顺序的一致性，即同一个批次的数据必须有同一个消费者来处理，这里用到了Kafka的partition的特性来保证消息的一致性和顺序。
- 为了提高消费者的吞吐能力，在消费端，采用多线程模型，即为每一种类型的数据里指定一个线程池来数据数据，较快数据的处理能力
- 为每批数据处理结果生成一个简要的处理概要，这里节后Prometheus和Grafana做数据视图
- 结合定时任务（PowerJob等），实现自动批处理。


## 设计图

### 架构图 ###
![](https://github.com/achui1980/data-exchange/blob/develop/diagram/architecture.png?raw=true)
### 数据流 ###
![](https://raw.githubusercontent.com/achui1980/data-exchange/develop/diagram/dataflow.png)

### 部署 (以AWS为例) ###

## 设计实现

- 模块化
  - [x] 将消费端和数据产生端分开成两个模块
  - [x] 数据模型独立成一个模块，供消费端和生产端使用
  - [x] 数据生产端处理后的数据进行对象序列化（当前都是String对象）
  - [x] 消费端能够自动识别生产端的对象，并进行反序列化

## 现有功能

- 从数据源获取数据

  - [x] SFTP
  - [x] API
- 数据源格式解析

  - [x] CSV
  - [x] Excel
  - [x] Json
  - [x] Memory
- 写入kafka

  - [x] Spring Kafa integration
- 写入redis

  - [x] Spring Redis ingegration
- 对接收到的每条数据进行处理

  - [x] 业务逻辑可以根据实际的需要进行编写
- 将redis的数据统一获取写入NFS

  - [x] 业务逻辑可以根据实际的需要进行编写
- 生成业务指标

  - [x] 任务处理时间

### 优化/改进 ###
- [x] 根据ActionId配置的配置文件统一从一处读取？
  - 挪出来统一处理。
- [x] Rest API 多次调用问题如何解决？
  -  将所有API请求全部塞入线程池，最后等待线程池所有任务执行完成，触发任务结束信号。（采用Guava的并发线程类库）
- [x] 如果多次调用，如何确保是同一批数据？
  - 任务开始时向消费端发送一条任务开始的标记，任务结束后再向消费者端发送一条任务结束的标记，开始和结束标记发送之前，所有的任务都在线程池处理。
- [ ] 数据消费过程中，处理逻辑发生异常，如何处理？是否重试？
